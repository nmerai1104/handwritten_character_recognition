{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XXbRj0K__qwP"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y0bPl6CpmrD-"
      },
      "outputs": [],
      "source": [
        "!pip install sklearn\n",
        "!pip install h5py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4Ge41kLtm8aq"
      },
      "outputs": [],
      "source": [
        "# Name of the file : Valueassign.py\n",
        "# Brief : Extracting the values from the word file file and removing the NAN values of the columns. Then assgining the words to a unique value.\n",
        "# Date : 27 March 2021\n",
        "# Important functions : pd.read_csv() - Using panda function to read the word file and give headers to all the columns\n",
        "#                       lamda x:x function and apply() function - USing both to assign the values of the last three columns and combine the words\n",
        "\n",
        "\n",
        "import pandas as pd\n",
        "# Delimiter the word file \n",
        "a = pd.read_csv('/content/drive/MyDrive/datadir/words_4.txt',names=[\"col1\",\"col2\",\"col3\",\"col4\",\"col5\",\"col6\",\"col7\", \"col8\",\"col9\",\"col10\",\"col11\"],low_memory=False, delimiter=\" \")\n",
        "# Printing the txt file length to see total words and cross check with the number of images\n",
        "print(len(a))\n",
        "\n",
        "# b = a[['col9', 'col10','col11']]\n",
        "# print(b) \n",
        "\n",
        "# Combining the rows of the columns using apply and lambda functions\n",
        "a['combined'] = a[['col9', 'col10','col11']].apply(lambda row: ' '.join(row.values.astype(str)), axis=1)\n",
        "\n",
        "# Removing the NaN values from the combined columns\n",
        "for i in range(len(a['combined'])):\n",
        "    a['combined'][i] = a['combined'][i].replace(\"nan\", \"\")\n",
        "    a['combined'][i] = a['combined'][i].replace(\" \", \"\")\n",
        "\n",
        "a['combined'].head()\n",
        "\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "# Lable Encoding\n",
        "le = LabelEncoder()\n",
        "a['combined_2'] = le.fit_transform(a['combined'])\n",
        "# getting unique value to the combine cols\n",
        "uniqueValues = a['combined_2'].unique()\n",
        "print(len(uniqueValues))\n",
        "outputCount = len(uniqueValues)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uBxtJRmxnIVo"
      },
      "outputs": [],
      "source": [
        "# Name of the file : model.py\n",
        "# Brief : Loading data and training the dataset in small batches\n",
        "# Date : 4 April 2021\n",
        "# Important functions : models.Sequential() - Grouping linear stack of layers\n",
        "#                       model.compile() - Configure model for training\n",
        "#                       model.summary() - Summarizing the model and its parameters\n",
        "#                       model.load_weights() - Loading weights of the model\n",
        "#                       flow_from_directory() - Takes the path to a directory & generates batches of augmented data\n",
        "#                       model.fit() - Trains the model for a fixed number of epochs\n",
        "#                       model.save() - save the model\n",
        "\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import models,layers\n",
        "import keras\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "img_height = 256\n",
        "img_width = 256\n",
        "batch_size = 18\n",
        "start = 0\n",
        "end=batch_size\n",
        "datagen = tf.keras.preprocessing.image.ImageDataGenerator()\n",
        "\n",
        "\n",
        "model = models.Sequential()\n",
        "model.add(layers.Conv2D(128, (3,3), activation='relu', input_shape=(img_height,img_width, 3)))\n",
        "model.add(layers.MaxPooling2D((2, 2)))\n",
        "model.add(layers.Conv2D(64, (3,3), activation='relu'))\n",
        "model.add(layers.MaxPooling2D((2, 2)))\n",
        "model.add(layers.Conv2D(32, (3,3), activation='relu'))\n",
        "model.add(layers.Flatten())\n",
        "\n",
        "model.add(layers.Dense(128, activation='relu'))\n",
        "\n",
        "model.add(layers.Dense(outputCount))\n",
        "model.compile(optimizer='adam',\n",
        "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "model.summary()\n",
        "\n",
        "model.load_weights('/content/drive/MyDrive/data_dir/model_history.h5')\n",
        "\n",
        "for j in range(100):\n",
        "    print(\"Epochs \"+str(j))\n",
        "    start = 0\n",
        "    end = batch_size\n",
        "    # Generating batches of augmented data\n",
        "    train_ds = datagen.flow_from_directory(\"/content/drive/MyDrive/imgs\", batch_size=batch_size,shuffle=False)\n",
        "    for i in range(int(train_ds.samples / batch_size) ):\n",
        "        # print(a['col1'][start:end])\n",
        "        x_train = train_ds.next()[0]\n",
        "        y_train = a['combined_2'][start:end]\n",
        "        history = model.fit(x_train,y_train, epochs=1)\n",
        "        start = start + batch_size\n",
        "        end = end + batch_size\n",
        "      \n",
        "model.save('/content/drive/MyDrive/data_dir/model_history.h5')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jaPeVQFAU1b9"
      },
      "outputs": [],
      "source": [
        "# Name of the file : output.py\n",
        "# Brief : \n",
        "# Date : 13 April 2021\n",
        "# Important functions : np.argmax() - Returns the indices of the maximum values along an axis of combine_2.\n",
        "\n",
        "import cv2\n",
        "img = cv2.imread(\"/content/test.png\")\n",
        "dim = (img_width, img_height)\n",
        "# resize image\n",
        "img = cv2.resize(img, dim).reshape(1,256,256,3)\n",
        "i = 0\n",
        "for j in range(len(a.index[a[\"combined_2\"] == np.argmax(model.predict(img).tolist())])):\n",
        "        if i == 0:\n",
        "          print(f' Prediction {a[\"combined\"][j]}')\n",
        "          i = 1\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "machine_shape": "hm",
      "name": "TechPRoject.py",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
