{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XXbRj0K__qwP"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y0bPl6CpmrD-"
      },
      "outputs": [],
      "source": [
        "!pip install sklearn\n",
        "!pip install h5py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4Ge41kLtm8aq"
      },
      "outputs": [],
      "source": [
        "# Name of the file : Valueassign.py\n",
        "# Brief : Extracting the values from the word file file and removing the NAN values of the columns. Then assgining the words to a unique value.\n",
        "# Important functions : pd.read_csv() - Using panda function to read the word file and give headers to all the columns\n",
        "#                       lamda x:x function and apply() function - USing both to assign the values of the last three columns and combine the words\n",
        "\n",
        "\n",
        "import pandas as pd\n",
        "# Delimiter the word file \n",
        "a = pd.read_csv('/content/drive/MyDrive/datadir/words_4.txt',names=[\"col1\",\"col2\",\"col3\",\"col4\",\"col5\",\"col6\",\"col7\", \"col8\",\"col9\",\"col10\",\"col11\"],low_memory=False, delimiter=\" \")\n",
        "# Printing the txt file length to see total words and cross check with the number of images\n",
        "print(len(a))\n",
        "\n",
        "# b = a[['col9', 'col10','col11']]\n",
        "# print(b) \n",
        "\n",
        "# Combining the rows of the columns using apply and lambda functions\n",
        "a['combined'] = a[['col9', 'col10','col11']].apply(lambda row: ' '.join(row.values.astype(str)), axis=1)\n",
        "\n",
        "# Removing the NaN values from the combined columns\n",
        "for i in range(len(a['combined'])):\n",
        "    a['combined'][i] = a['combined'][i].replace(\"nan\", \"\")\n",
        "    a['combined'][i] = a['combined'][i].replace(\" \", \"\")\n",
        "\n",
        "a['combined'].head()\n",
        "\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "# Lable Encoding\n",
        "le = LabelEncoder()\n",
        "a['combined_2'] = le.fit_transform(a['combined'])\n",
        "# getting unique value to the combine cols\n",
        "uniqueValues = a['combined_2'].unique()\n",
        "print(len(uniqueValues))\n",
        "outputCount = len(uniqueValues)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uBxtJRmxnIVo"
      },
      "outputs": [],
      "source": [
     import pandas as pd
import numpy as np
import cv2
import tensorflow as tf
from tensorflow.keras import models, layers
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from sklearn.preprocessing import LabelEncoder

# Load the word file
a = pd.read_csv('/content/drive/MyDrive/datadir/words_4.txt', names=["col1","col2","col3","col4","col5","col6","col7", "col8","col9","col10","col11"], low_memory=False, delimiter=" ")

# Combine the rows of the columns
a['combined'] = a[['col9', 'col10','col11']].apply(lambda row: ' '.join(row.values.astype(str)), axis=1)

# Remove the NaN values from the combined columns
a['combined'] = a['combined'].str.replace("nan", "").str.replace(" ", "")

# Label Encoding
le = LabelEncoder()
a['combined_2'] = le.fit_transform(a['combined'])
outputCount = len(le.classes_)

# Define image dimensions and batch size
img_height = 256
img_width = 256
batch_size = 18

# Data augmentation and image preprocessing
datagen = ImageDataGenerator(rescale=1.0/255.0)

# Define the CNN model
model = models.Sequential()
model.add(layers.Conv2D(128, (3,3), activation='relu', input_shape=(img_height,img_width, 3)))
model.add(layers.MaxPooling2D((2, 2)))
model.add(layers.Conv2D(64, (3,3), activation='relu'))
model.add(layers.MaxPooling2D((2, 2)))
model.add(layers.Conv2D(32, (3,3), activation='relu'))
model.add(layers.Flatten())
model.add(layers.Dense(128, activation='relu'))
model.add(layers.Dense(outputCount))

# Compile the model
model.compile(optimizer='adam',
              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
              metrics=['accuracy'])

model.summary()

# Training loop
epochs = 100
train_dir = "/content/drive/MyDrive/imgs"
train_ds = datagen.flow_from_directory(train_dir, target_size=(img_height, img_width), batch_size=batch_size, class_mode='sparse')

history = model.fit(train_ds, epochs=epochs)

# Save the trained model
model.save('/content/drive/MyDrive/data_dir/model_history.h5')

# Load the trained model
model = models.load_model('/content/drive/MyDrive/data_dir/model_history.h5')

# Load and preprocess the test image
test_image_path = "/content/test.png"
img = cv2.imread(test_image_path)
img = cv2.resize(img, (img_width, img_height))
img = img.reshape(1, img_height, img_width, 3)
img = datagen.standardize(img)

# Make predictions on the test image
prediction = model.predict(img)
predicted_class = np.argmax(prediction)
predicted_word = a["combined"][predicted_class]
print(f"Prediction: {predicted_word}")

      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jaPeVQFAU1b9"
      },
      "outputs": [],
      "source": [
        "# Name of the file : output.py\n",
        "# Brief : \n",
        "# Important functions : np.argmax() - Returns the indices of the maximum values along an axis of combine_2.\n",
        "\n",
        "import cv2\n",
        "img = cv2.imread(\"/content/test.png\")\n",
        "dim = (img_width, img_height)\n",
        "# resize image\n",
        "img = cv2.resize(img, dim).reshape(1,256,256,3)\n",
        "i = 0\n",
        "for j in range(len(a.index[a[\"combined_2\"] == np.argmax(model.predict(img).tolist())])):\n",
        "        if i == 0:\n",
        "          print(f' Prediction {a[\"combined\"][j]}')\n",
        "          i = 1\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "machine_shape": "hm",
      "name": "TechPRoject.py",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
