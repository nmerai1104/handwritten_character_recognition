{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "XXbRj0K__qwP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cb052839-eec1-40d4-a214-daa92436920e"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y0bPl6CpmrD-"
      },
      "source": [
        "!pip install sklearn\n",
        "!pip install h5py"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Ge41kLtm8aq"
      },
      "source": [
        "import pandas as pd\n",
        "# Delimiter the word file\n",
        "a = pd.read_csv('/content/drive/MyDrive/datadir/words_4.txt',names=[\"col1\",\"col2\",\"col3\",\"col4\",\"col5\",\"col6\",\"col7\", \"col8\",\"col9\",\"col10\",\"col11\"],low_memory=False, delimiter=\" \")\n",
        "# Printing the txt file length to see total words and cross check with the number of images\n",
        "print(len(a))\n",
        "\n",
        "# b = a[['col9', 'col10','col11']]\n",
        "# print(b)\n",
        "\n",
        "# Combining the rows of the columns using apply and lambda functions\n",
        "a['combined'] = a[['col9', 'col10','col11']].apply(lambda row: ' '.join(row.values.astype(str)), axis=1)\n",
        "\n",
        "# Removing the NaN values from the combined columns\n",
        "for i in range(len(a['combined'])):\n",
        "    a['combined'][i] = a['combined'][i].replace(\"nan\", \"\")\n",
        "    a['combined'][i] = a['combined'][i].replace(\" \", \"\")\n",
        "\n",
        "a['combined'].head()\n",
        "\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "# Lable Encoding\n",
        "le = LabelEncoder()\n",
        "a['combined_2'] = le.fit_transform(a['combined'])\n",
        "# getting unique value to the combine cols\n",
        "uniqueValues = a['combined_2'].unique()\n",
        "print(len(uniqueValues))\n",
        "outputCount = len(uniqueValues)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uBxtJRmxnIVo"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import cv2\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import models, layers\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# Load the word file\n",
        "a = pd.read_csv('/content/drive/MyDrive/datadir/words_4.txt', names=[\"col1\",\"col2\",\"col3\",\"col4\",\"col5\",\"col6\",\"col7\", \"col8\",\"col9\",\"col10\",\"col11\"], low_memory=False, delimiter=\" \")\n",
        "\n",
        "# Combine the rows of the columns\n",
        "a['combined'] = a[['col9', 'col10','col11']].apply(lambda row: ' '.join(row.values.astype(str)), axis=1)\n",
        "\n",
        "# Remove the NaN values from the combined columns\n",
        "a['combined'] = a['combined'].str.replace(\"nan\", \"\").str.replace(\" \", \"\")\n",
        "\n",
        "# Label Encoding\n",
        "le = LabelEncoder()\n",
        "a['combined_2'] = le.fit_transform(a['combined'])\n",
        "outputCount = len(le.classes_)\n",
        "\n",
        "# Define image dimensions and batch size\n",
        "img_height = 256\n",
        "img_width = 256\n",
        "batch_size = 18\n",
        "\n",
        "# Data augmentation and image preprocessing\n",
        "datagen = ImageDataGenerator(rescale=1.0/255.0)\n",
        "\n",
        "# Define the CNN model\n",
        "model = models.Sequential()\n",
        "model.add(layers.Conv2D(128, (3,3), activation='relu', input_shape=(img_height,img_width, 3)))\n",
        "model.add(layers.MaxPooling2D((2, 2)))\n",
        "model.add(layers.Conv2D(64, (3,3), activation='relu'))\n",
        "model.add(layers.MaxPooling2D((2, 2)))\n",
        "model.add(layers.Conv2D(32, (3,3), activation='relu'))\n",
        "model.add(layers.Flatten())\n",
        "model.add(layers.Dense(128, activation='relu'))\n",
        "model.add(layers.Dense(outputCount))\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam',\n",
        "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "model.summary()\n",
        "\n",
        "# Training loop\n",
        "epochs = 100\n",
        "train_dir = \"/content/drive/MyDrive/imgs\"\n",
        "train_ds = datagen.flow_from_directory(train_dir, target_size=(img_height, img_width), batch_size=batch_size, class_mode='sparse')\n",
        "\n",
        "history = model.fit(train_ds, epochs=epochs)\n",
        "\n",
        "# Save the trained model\n",
        "model.save('/content/drive/MyDrive/data_dir/model_history.h5')\n",
        "\n",
        "# Load the trained model\n",
        "model = models.load_model('/content/drive/MyDrive/data_dir/model_history.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jaPeVQFAU1b9"
      },
      "source": [
        "# Load and preprocess the test image\n",
        "test_image_path = \"/content/test.png\"\n",
        "img = cv2.imread(test_image_path)\n",
        "img = cv2.resize(img, (img_width, img_height))\n",
        "img = img.reshape(1, img_height, img_width, 3)\n",
        "img = datagen.standardize(img)\n",
        "\n",
        "# Make predictions on the test image\n",
        "prediction = model.predict(img)\n",
        "predicted_class = np.argmax(prediction)\n",
        "predicted_word = a[\"combined\"][predicted_class]\n",
        "print(f\"Prediction: {predicted_word}\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
